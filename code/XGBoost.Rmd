---
title: "Prediction with XGBoost"
output: html_document
date: "2025-10-29"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
set.seed(42) # ensures reproducibility across all chunks
library(readr)
library(dplyr)
library(xgboost)
library(pROC)
library(PRROC)
library(ggplot2)
library(Matrix)
library(stringr)
library(knitr)
library(kableExtra)
library(webshot2)
library(shapviz)
```


# 1. Loading and Inspecting the Data

```{r}
# setting directory (probably needs to be changed later)
getwd()
setwd("C:\\Users\\jordy\\OneDrive\\Documents\\Coach_Scott\\Injuries")
dat <- read.csv("lasso_selected_43vars.csv")
str(dat)
```

# 2. Splitting into Training and Testing Set

```{r}
train_idx <- caret::createDataPartition(dat$Injured, p = 0.80, list = FALSE)
train <- dat[train_idx, ]
test  <- dat[-train_idx, ]

# Matrix form for XGBoost (guarantee numeric)
x_train <- train |> dplyr::select(-Injured) |> data.matrix()
y_train <- train$Injured

x_test  <- test  |> dplyr::select(-Injured) |> data.matrix()
y_test  <- test$Injured
```

We split the data into a training (80%) and testing (20%) set just like in feature selection. Again, we do not manually standardize the variables because tree-based methods are scale-invariant. 

# 3. Handling Class Imbalance Using scale_pos_weight

As described before, we are dealing with approximately 6% of the jumpes classified as "Injured," making it highly imbalanced. We will now compute the imbalance ratio and set up the XGBoost DMatrix objects. 

```{r}
# counting number of injured (1) and non-injured (0) jumps in the training data
pos <- sum(y_train == 1)
neg <- sum(y_train == 0)

# calculating the ration for XGBoost's weighting
scale_pos_weight <- neg / pos

# printing out the imbalance info
cat("Injured (positive):", pos, "\nNon-injured (negative):", neg,
    "\nscale_pos_weight =", round(scale_pos_weight, 2), "\n")
```

In the training data, only 1,103 out of all training jumps are injury events. So, each injured jump will be treated as roughly 18 times more important than non-injured jumps during model training. This balances the gradient updates so the model pays attention to both classes equally. 

```{r}
# putting this output in a clean table format for report

# summarize imbalance results
imbalance_table <- data.frame(
  Class = c("Non-Injured (0)", "Injured (1)", "scale_pos_weight"),
  Count = c(neg, pos, round(scale_pos_weight, 2))
)

# format numeric cells neatly
imbalance_table$Count <- format(imbalance_table$Count, big.mark = ",")

# create a temporary HTML file of the table
table_file <- "table1_class_imbalance.html"
image_file <- "table1_class_imbalance.png"

kbl(imbalance_table,
    caption = "Table 1: Class Imbalance Summary — Training Set Only",
    col.names = c("Class / Parameter", "Count"),
    align = "lc",
    format = "html") %>%
  kable_styling(full_width = FALSE, position = "center", font_size = 12) %>%
  row_spec(0, bold = TRUE, background = "#f2f2f2") %>%
  column_spec(1, bold = TRUE) %>%
  column_spec(2, color = "white", background = "#D02130") %>%
  save_kable(file = table_file)

# convert the HTML table to a PNG image
webshot2::webshot(table_file, file = image_file, vwidth = 800, vheight = 600)

cat("✅ Saved as:", image_file, "\n")
```

# 4. Building DMatrix Objects (Required by XGBoost)

```{r}
dtrain <- xgboost::xgb.DMatrix(data = x_train, label = y_train)
dtest  <- xgboost::xgb.DMatrix(data = x_test,  label = y_test)
watch  <- list(train = dtrain, eval = dtest)
```

# 5. Cross-Validation and Parameter Tuning

```{r}
# each row is one candidate configuration to try in CV.
param_grid <- expand.grid(
  eta = c(0.05, 0.10),        # learning rate: smaller = slower but safer training
  max_depth = c(3, 5, 7),     # tree depth: higher can fit more complex interactions
  min_child_weight = c(1, 5), # min sum of instance weight in a child: higher = more conservative splits
  subsample = c(0.7, 0.9),    # row subsampling per tree: <1 adds randomness & reduces overfitting
  colsample_bytree = c(0.7, 0.9) # column subsampling per tree: same idea, for features
)

nrow(param_grid) # checking dimensions to see if it worked

# object to store the best result we see across the grid
best <- list(auc = -Inf, params = NULL, nrounds = NULL, i = NA)
```

We will now loop through the grid. For each configuration we run 5-fold cross-validation on the training set, optimize AUC, and use early stopping to pick the best number of trees (nrounds). We will keep the best model seen so far. 

```{r}
dir.create("results", showWarnings = FALSE)

cv_summaries <- vector("list", nrow(param_grid))  # storing the results

if (file.exists("results/xgb_best_model_info.rds") &&
    file.exists("results/xgb_cv_results.rds")) {
  best       <- readRDS("results/xgb_best_model_info.rds")
  cv_results <- readRDS("results/xgb_cv_results.rds")
  message("✅ Loaded saved CV results. Skipping grid search.")
} else {
for (i in seq_len(nrow(param_grid))) {
  p <- param_grid[i, ]

  params <- list(
    objective        = "binary:logistic",
    eval_metric      = "auc",
    eta              = p$eta,
    max_depth        = p$max_depth,
    min_child_weight = p$min_child_weight,
    subsample        = p$subsample,
    colsample_bytree = p$colsample_bytree,
    lambda           = 1,
    alpha            = 0,
    scale_pos_weight = scale_pos_weight,  # from Step 3
    tree_method      = "hist"
  )

  message(sprintf(
    "CV %02d/%02d | eta=%.2f, depth=%d, mcw=%d, subs=%.2f, cols=%.2f",
    i, nrow(param_grid), p$eta, p$max_depth, p$min_child_weight, p$subsample, p$colsample_bytree
  ))

  cv <- tryCatch(
    xgboost::xgb.cv(
      params = params,
      data = dtrain,
      nrounds = 2000,            # upper bound; early stopping picks the winner
      nfold = 5,
      stratified = TRUE,
      early_stopping_rounds = 50,
      verbose = 0
    ),
    error = function(e) {
      warning(sprintf("Skipping combo %d due to error: %s", i, e$message))
      return(NULL)
    }
  )
  if (is.null(cv)) next

  auc_vec   <- cv$evaluation_log$test_auc_mean
  mean_auc  <- max(auc_vec, na.rm = TRUE)
  best_iter <- cv$best_iteration
  if (is.null(best_iter) || is.na(best_iter)) best_iter <- which.max(auc_vec)

  cv_summaries[[i]] <- data.frame(
    i = i,
    eta = p$eta,
    max_depth = p$max_depth,
    min_child_weight = p$min_child_weight,
    subsample = p$subsample,
    colsample_bytree = p$colsample_bytree,
    best_auc = mean_auc,
    best_nrounds = best_iter
  )

  if (mean_auc > best$auc) {
    best <- list(auc = mean_auc, params = params, nrounds = best_iter, i = i)
  }

  message(sprintf("  -> combo AUC: %.4f at %d rounds", mean_auc, best_iter))
}
}
```

We'll save the cross validation results to avoid rerunning the for-loop again. 

```{r}
cv_results <- dplyr::bind_rows(cv_summaries[!sapply(cv_summaries, is.null)])
  saveRDS(cv_results, "results/xgb_cv_results.rds")
  saveRDS(best,       "results/xgb_best_model_info.rds")
  message("✅ Saved: results/xgb_cv_results.rds and results/xgb_best_model_info.rds")
```

# 6. Train Final XGBoost Model Using Best Parameters

We will now fit the final model using the optimal hyperparameters identified from the cross-validation search. We'll train on the entire training set, using best$nrounds for the number of boosting iterations, and then evaluate performance on the test set. 

```{r}
# extracting best parameters and number of trees
final_params  <- best$params
final_nrounds <- best$nrounds

cat("Best Parameters:\n")
print(final_params)
cat("\nOptimal Number of Boosting Rounds:", final_nrounds, "\n")
```

The above output shows us that the cross-validation process decided 756 trees was the optimal stopping point, where after that test AUC stopped improving. This is to be expected given our small learning rate of 0.05. 
```{r}
# train the final model on full training set 
final_model <- xgboost::xgb.train(
  params  = final_params,
  data    = dtrain,
  nrounds = final_nrounds,
  watchlist = watch,         
  verbose = 0 # hiding the long output because there will be 756 prints
)

# Save model as .rds for future reproducibility
saveRDS(final_model, "results/final_xgb_model.rds")
cat("✅ Final model saved as results/final_xgb_model.rds \n")
```

The above evaluates the model on the entire training set. 

# 7. Evaluating the Final XGBoost Model on Test Data

We will begin by getting the trained model to estimate probabilities that each test jump is an injured event (Injured = 1). 

```{r}
# testing predicted probabilities for the positive class (Injured)
test_pred_prob <- predict(final_model, dtest)
head(test_pred_prob)
```

We will now build the ROC curve and compute AUC score. 

```{r}
# compute ROC
roc_obj <- pROC::roc(response = y_test, predictor = test_pred_prob)

# AUC
auc_value <- pROC::auc(roc_obj)

# display
cat("AUC =", round(auc_value, 3), "\n")

# ROC curve plot
plot(
  roc_obj,
  main = "ROC Curve for XGBoost - Test Set",
  print.auc = TRUE,
  legacy.axes = TRUE
)
```

The above ROC curve shows the performance of the XGBoost model on the test set. The X-axis shows specificity, or the false positive rate. Lower values are better, meaning less of the non-injured jumps are incorrectly classified as "injured". The Y-axis shows the true positive rate, meaning the porportion of actual injuries correctly identified. Higher values mean the model catches more true injures. Because our curve stays well above the diagonal "random guessing" line with an AUC of 0.830, we can say in 83% of randomly paired injured vs. non-injured jumps, the injured jump is scored higher. This is strong predictive performance, especially given the imbalanced injury data. 

We will now convert probabilities to class labels with a threshold of 0.5. We will rerun this later using a data-driven threshold like the F1 optimal value. 

```{r}
# converting probabilities to 0/1 class predictions using 0.5 cutoff
test_pred_class <- ifelse(test_pred_prob >= 0.5, 1, 0)
```

Next, we'll compute confusion matrix, accuracy, precision, recall and F1. 

```{r}
# confusion matrix components
tn <- sum(test_pred_class == 0 & y_test == 0)
tp <- sum(test_pred_class == 1 & y_test == 1)
fn <- sum(test_pred_class == 0 & y_test == 1)
fp <- sum(test_pred_class == 1 & y_test == 0)

# metrics
accuracy  <- (tp + tn) / (tp + tn + fp + fn)
precision <- tp / (tp + fp)
recall    <- tp / (tp + fn)   # a.k.a sensitivity or true positive rate
f1_score  <- 2 * (precision * recall) / (precision + recall)

# print in clean format
metrics <- data.frame(
  Metric = c("Accuracy", "Precision", "Recall (Sensitivity)", "F1 Score"),
  Value  = round(c(accuracy, precision, recall, f1_score), 3)
)
metrics

```

From the above, we see that when a model predicts an injury, it is right only about 36% of the time. Further, the model only catches about 25% of actual injuries. Finally, our F1 score is low indicating an imbalance. Our model predicts "not injured" for everything which explains our high accuracy value. We will now find the best cutoff by maximizing F1 score. 

```{r}
# create all possible thresholds from 0 to 1
thresholds <- seq(0, 1, by = 0.01)

# for each threshold, calculate precision, recall, F1
f1_scores <- sapply(thresholds, function(t) {
  pred <- ifelse(test_pred_prob >= t, 1, 0)
  
  tp <- sum(pred == 1 & y_test == 1)
  fp <- sum(pred == 1 & y_test == 0)
  fn <- sum(pred == 0 & y_test == 1)
  
  precision <- ifelse(tp + fp == 0, 0, tp / (tp + fp))
  recall    <- ifelse(tp + fn == 0, 0, tp / (tp + fn))
  
  if (precision + recall == 0) return(0)
  return(2 * precision * recall / (precision + recall))  # F1 score
})

# get best cutoff
best_idx <- which.max(f1_scores)
best_threshold <- thresholds[best_idx]
best_f1 <- f1_scores[best_idx]

cat("Optimal threshold =", best_threshold, "with F1 Score =", round(best_f1, 3), "\n")

```

We will now evaluate the model using this new cutoff rather than 0.5. 

```{r}
# apply best threshold
test_pred_best <- ifelse(test_pred_prob >= best_threshold, 1, 0)

# recompute confusion matrix metrics
tp <- sum(test_pred_best == 1 & y_test == 1)
tn <- sum(test_pred_best == 0 & y_test == 0)
fp <- sum(test_pred_best == 1 & y_test == 0)
fn <- sum(test_pred_best == 0 & y_test == 1)

# metrics
accuracy_best  <- (tp + tn) / (tp + tn + fp + fn)
precision_best <- tp / (tp + fp)
recall_best    <- tp / (tp + fn)
f1_best        <- 2 * (precision_best * recall_best) / (precision_best + recall_best)

data.frame(
  Threshold = round(best_threshold, 3),
  Accuracy = round(accuracy_best, 3),
  Precision = round(precision_best, 3),
  Recall = round(recall_best, 3),
  F1 = round(f1_best, 3)
)

```

With a new F1 score of 0.372, we have a better balance between precision and recall. We are effectively making the model more sensitive to catch more injuries at the cost of flagging a few more false positives. With this new threshold of 0.21, our precision slightly decreased but our recall increased substantially and our F1 score improved. This is exactly the case we want in order to catch more injuries. We will now create a precision-recall curve which may be better than ROC for imbalanced data. 

We will now create a confusion matrix table using this optimal cutoff point. 

```{r}
# predictions using this optimal threshold
test_pred_best <- ifelse(test_pred_prob >= best_threshold, 1, 0)

# compute components of confusion matrix
tn <- sum(test_pred_best == 0 & y_test == 0)
fp <- sum(test_pred_best == 1 & y_test == 0)
fn <- sum(test_pred_best == 0 & y_test == 1)
tp <- sum(test_pred_best == 1 & y_test == 1)

# put into a table format
confusion_table <- data.frame(
  Actual = c("Non-Injured (0)", "Injured (1)"),
  `Predicted 0` = c(tn, fn),
  `Predicted 1` = c(fp, tp)
)

# format into table in the same style as the others
kbl(confusion_table,
    caption = paste0("Table X: Confusion Matrix at Optimal Threshold (", round(best_threshold, 2), ")"),
    col.names = c("Actual Class", "Predicted 0", "Predicted 1"),
    align = "lcc",
    format = "html") %>%
  kable_styling(full_width = FALSE, position = "center", font_size = 12) %>%
  row_spec(0, bold = TRUE, background = "#f2f2f2") %>%
  column_spec(1, bold = TRUE) %>%
  column_spec(2:3, color = "white", background = "#D02130")  

```

From the confusion matrix, we see the model correctly identifies the vast majority of non-injure jumps as "not injured". This is expected since the dataset is highly imbalanced. In the case of injury prevention, we aren't necessarily as concerned with the false positives (257). The false negatives (159) are the missed injury cases. These are the most concerning, because missing an athlete at risk could lead to real injury. With this threshold, the true positives were 123, meaning the model catches 44% of all injuries (recall). 

At the optimal threshold of 0.21 (chosen to maximize the F1-score), the model correctly classified 123 out of 282 true injury cases (Sensitivity = 43.6%). Although accuracy slightly decreased from 93.5% to 92.0%, recall significantly improved, meaning the model identifies more athletes at risk of injury which is more important in this context than overall accuracy.

Because of the imbalance in our dataset, we will now create the Precision-Recall Curve which gives us more information than the ROC curve. 
```{r}
# compute precision-recall curve data
pr_obj <- pr.curve(
  scores.class0 = test_pred_prob[y_test == 1],  # predicted probabilities for actual injuries
  scores.class1 = test_pred_prob[y_test == 0],
  curve = TRUE
)

# plot Precision-Recall curve
plot(
  pr_obj,
  main = "Precision–Recall Curve for XGBoost (Test Set)",
  auc.main = FALSE,
  lwd = 2,
  col = "#D02130"
)

# add AUPRC (area under precision-recall curve)
cat("AUPRC =", round(pr_obj$auc.integral, 3), "\n")
```

The Precision–Recall curve above demonstrates how the model’s ability to correctly identify injured jumps (recall) trades off with its correctness when it predicts an injury (precision). Because injuries make up only 6% of the data, the PR curve is a more informative diagnostic than the ROC curve. Our model achieves a maximum F1 score at threshold 0.21, where precision = 0.324 and recall = 0.436. This means the model correctly identifies ~43.6% of injuries while being correct about ~32.4% of its injury predictions—approximately five times better than random guessing. Performance degrades as recall increases beyond ~0.4, showing that the model cannot catch all injuries without incurring many false alarms.


# SHAP Value Computation and Interpretation

We will begin by computing SHAP values on the test set to help explain why the mdoel makes each prediction and which CMJ variables contribute most to injury risk predictions. 

```{r}
# SHAP contributions for each test row and feature (+ a BIAS/base value column)
shap_test <- predict(final_model, dtest, predcontrib = TRUE)

# put into a data frame and attach feature names
shap_test <- as.data.frame(shap_test)
colnames(shap_test) <- c(colnames(x_test), "BIAS")

# drop the baseline (intercept) column for importance summaries
shap_only <- shap_test[, setdiff(names(shap_test), "BIAS")]

```

We will now rank features by how influential they are overall, ignoring positive or negative influence on injury risk by taking the mean absolute SHAP. 

```{r}
shap_importance <- shap_only |>
  summarize(across(everything(), ~ mean(abs(.), na.rm = TRUE))) |>
  tidyr::pivot_longer(everything(),
                      names_to = "Feature",
                      values_to = "MeanAbsSHAP") |>
  arrange(desc(MeanAbsSHAP))

head(shap_importance, 10)
```

The variables shown above cause the largest changes in predicted injury risk. Not necessarily higher/lower but influence either way. 

```{r}
topN <- 15

# only keep the first two columns — "Feature" and "Mean|SHAP|"
imp_tbl <- shap_importance[1:topN, 1:2]

kbl(
  imp_tbl,
  caption = "Table X: Global Feature Importance via Mean |SHAP| (Test Set)",
  col.names = c("Feature", "Mean |SHAP| (log-odds)"),
  align = c("l", "r"),  
  digits = 4,
  format = "html"
) |>
  kable_styling(full_width = FALSE, position = "center", font_size = 12) |>
  row_spec(0, bold = TRUE, background = "#f2f2f2") |>
  column_spec(1, bold = TRUE) |>
  column_spec(2, color = "white", background = "#D02130")

```

This table displays the top 15 CMJ metrics with the most influence on injury prediction as described in our XGBoost model. 8 of which come from the propulsive phase of the jump with the most coming from average propulsive velocity. 


