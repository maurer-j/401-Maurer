---
title: "Feature_Selection"
output: html_document
date: "2025-10-13"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
set.seed(42) # ensures reproducibility across all chunks
library(readr)
library(dplyr)
library(corrplot)
library(reshape2)
library(ggplot2)
library(ggcorrplot)
library(glmnet)
library(caret)
library(pROC)
library(broom)
```



# 1. Loading and Inspecting Data
```{r}
# setting directory (probably needs to be changed later)
getwd()
setwd("C:\\Users\\jordy\\OneDrive\\Documents\\Coach_Scott\\Injuries")
data <- read.csv("final_data.csv")
```

```{r}
# checking data structure
str(data)

summary(data$Injured)

# making sure INjured is numeric
data$Injured <- as.numeric(data$Injured)
```

Now, we need to remove identifiers and non-numeric colummns. These identifiers include IDs, organizations, dates, and injury types.We will remove all character variables as well as "days_before" which was a helper variable to identify injured vs. non-injured jumps.  

```{r}
# removing identifiable information that will not be involved in the study and keeping only numeric data.  
data_numeric <- data %>%
  select(where(~ !is.character(.))) %>%
  select(-days_before)

str(data_numeric)
```

# 2. Exploring Correlations Among CMJ Metrics

```{r}
# initial plot with all metrics
corr_matrix <- cor(data_numeric %>% select(-Injured), use = "pairwise.complete.obs")
corrplot(corr_matrix, method = "color", tl.cex = 0.5)
```

From the above heatmap, we see there are an overwhelming number of metrics to compare all at once. Instead of plotting them all at once, we will instead find only the metrics with a relation greater than 0.8. 

```{r}
# filtering values above 0.8
corr_long <- melt(corr_matrix) %>%
  filter(Var1 != Var2, abs(value) > 0.8) %>%  
  arrange(desc(abs(value)))

corr_long # viewing

# seeing how many unique variables are in this output
unique_vars <- unique(c(corr_long$Var1, corr_long$Var2))
length(unique_vars)
```

Above, we've created a table of variable pairs that are highly correlated. We see that there are 70 unique variables correlated with each other. This is what we expected from the CMJ in which each movement relates to the next, showing lots of correlated metrics. We will now create a histogram to demonstrate the spread of correlation values. 

```{r}
# creating histogram of correlations
corr_values <- melt(corr_matrix)$value

corr_coeff <- ggplot(data.frame(corr_values), aes(x = corr_values)) +
  geom_histogram(bins = 50, fill = "#D02130", color = "white") +
  labs(
    title = "Distribution of Pairwise Correlations Among CMJ Metrics",
    x = "Correlation Coefficient (r)",
    y = "Count"
  ) +
  theme_minimal()
corr_coeff

ggsave("correlation_coefficients_his.png", plot = corr_coeff, width = 6, height = 4, units = "in", dpi = 300)
```

The above visual demonstrates that many CMJ metrics are strongly interrelated, while others remain only weakly associated. The clustering of high positive correlations reflects the biomechanicial origins of the derivation of the CMJ metrics corresponding with particular phases. This widespread multicollinearity highlights the need for penalized regression methods that can manage correlated predictors and isolate those that provide unique information about injury risk. 

# 3. Elastic Net Classification for Feature Selection

We will now use Elastic Net that blends lasso ($\alpha = 1$) and ridge ($\alpha = 0$) methods, accounting for multicollinearity and variable selection. 

```{r}
# performing an 80/20 split by athlete ID for train and test 
ids <- unique(data$ID)
train_ids <- sample(ids, size = floor(0.80 * length(ids)))
is_train <- data$ID %in% train_ids

train_df <- data_numeric[is_train, ]
test_df <- data_numeric[!is_train, ]
```

Before running elastic net, we are going to check for missing values that may affect the process. 

```{r}
# counting NA values per column in training set
na_counts <- colSums(is.na(train_df))

# displaying only columns that actually have missing values
na_counts[na_counts > 0]

# checking NA percentages
na_percent <- (na_counts / nrow(train_df)) * 100
na_percent[na_percent > 0]  # only show columns with missing data

```

From the above output, we see there are 12 columns with 0.16% missing values. To account for these, we will insert the median of the column wherever the missing value occurs. 

```{r}
# median imputation

# identifying numeric columns (excluding Injured)
num_cols <- setdiff(names(train_df), "Injured")

# computing medians from the training data
meds <- sapply(train_df[, num_cols, drop = FALSE],
               function(x) median(x, na.rm = TRUE))

# replacing NA values in the training and test sets with those medians
for (nm in num_cols) {
  # Impute in training data
  train_df[[nm]][is.na(train_df[[nm]])] <- meds[[nm]]
  
  # Impute in test data (using training medians!)
  if (nm %in% names(test_df)) {
    test_df[[nm]][is.na(test_df[[nm]])] <- meds[[nm]]
  }
}

# checking to see that it worked (should return zero for both)
sum(is.na(train_df))
sum(is.na(test_df))
```

We now have removed any NA values from the training and test sets that may influence our Elastic Net Classification. We will now create the model matrices and scale our predictors.

```{r}
# creating predictor matrix (x) and outcome vector (y)
x_train <- model.matrix(Injured ~ ., data = train_df)[, -1]
y_train <- train_df$Injured

x_test  <- model.matrix(Injured ~ ., data = test_df)[, -1]
y_test  <- test_df$Injured
```

Next, we'll search for the optimal value of $\alpha$ with 10-fold cross-validation, using AUC as our judge of model performance.

## 3.1 Finding the optimal alpha ($\alpha$) for Elastic Net

```{r}
# defining a range of alpha values from ridge (0) to lasso (1)
alphas <- seq(0, 1, by = 0.1)

cv_results <- data.frame(alpha = alphas, mean_AUC = NA)

for (i in seq_along(alphas)) {
  cv <- cv.glmnet(
    x_train, y_train,
    family = "binomial",
    alpha = alphas[i],
    type.measure = "auc", 
    nfolds = 10,
    parallel = TRUE
  )
  cv_results$mean_AUC[i] <- max(cv$cvm)
}

cv_results
```

We will plot the results for readability. 

```{r}
best_idx <- which.max(cv_results$mean_AUC)
best_alpha <- cv_results$alpha[best_idx]
best_auc <- cv_results$mean_AUC[best_idx]

alpha_plot <- ggplot(cv_results, aes(x = alpha, y = mean_AUC)) +
  geom_line(color = "#1f77b4", linewidth = 1) +
  geom_point(size = 3, color = "#1f77b4") +
  geom_vline(xintercept = best_alpha, linetype = "dashed", color = "red", linewidth = 0.8) +
  annotate(
    "text",
    x = best_alpha + 0.1,   # move text to the right
    y = best_auc + 0.002,   # move text slightly above line
    label = paste0("Best α = ", round(best_alpha, 2), "\nAUC = ", round(best_auc, 3)),
    color = "red",
    size = 3,
    hjust = 0
  ) +
  labs(
    title = "Mean AUC Across Alpha Values in Elastic Net Cross-Validation",
    subtitle = "Evaluating Elastic Net balance between Ridge (α = 0) and Lasso (α = 1)",
    x = expression(alpha ~ "(Penalty Mix: Ridge → Lasso)"),
    y = "Mean AUC (10-Fold CV)"
  ) +
  theme_minimal(base_size = 12) +
  theme(
    plot.title = element_text(face = "bold", size = 12),
    plot.subtitle = element_text(size = 10, color = "gray30"),
    axis.title = element_text(face = "bold", size = 9),
    panel.grid.minor = element_blank(),
    panel.grid.major.x = element_blank()
  )
alpha_plot

ggsave("optimal_alpha_values.png", plot = alpha_plot, width = 6, height = 4, units = "in", dpi = 300)
```

From the above output, we are shown how the mean AUC varies across diferent values of the mixing parameter, $/alpha $. Model performance peaked at $/alpha = 0.1$ (AUC = 0.646), indicating a structure more similar to Ridge regression. This makes sense considering many of the variables are highly correlated. 

```{r}
# creating a variable for the optimal alpha value
best_alpha <- cv_results$alpha[which.max(cv_results$mean_AUC)]
best_alpha
```

Since we have found the optimal $/alpha$ value, we will now find the best $/lambda$ (penalty) using 10-fold cross-validation. 

## 3.2 Fitting Elastic Net with the Optimal Alpha

```{r}
# finding the best lambda
cv_elnet <- cv.glmnet(
  x_train, y_train,
  family = "binomial",
  alpha = best_alpha,
  type.measure = "auc",
  nfolds = 10
)

png("elasticnet_plot.png", width = 800, height = 600, res = 150)
plot(cv_elnet)
title(main = paste("Elastic Net Cross-Validation (α =", best_alpha, ")"), line = 2.5)
```

The figure above shows the results of 10-fold cross-validation used to determinal the optimal penalty parameter for the model at alpha = 0.1. Each red point represents the mean AUC for a given lambda value, while the grey bars indicate the standard errors. The left-dashed line represents the value that produces the highest average AUC. The right-dashed line is the largest lambda within one standard error of the minimum, which provides a similar performance with fewer predictors. We will go forward with the minimum lambda value. 

We will now extract this lamdba value and get coefficients for the best alpha/lambda combination. 

```{r}
# getting best lambda
best_lambda <- cv_elnet$lambda.min
best_lambda

# getting coefficients for the best alpha/lambda combo
coef_elnet <- coef(cv_elnet, s = "lambda.min")
coef_elnet
```

Because our alpha value leaned more towards Ridge, there was limited feature selection that took place after using the minimum lambda value. It shrinks the coefficients but does not zero them out. However, we want a smaller set of predictors to feed into a predictive model. Since $/alpha = 0.1$ was too similar to Ridge regression in handling multicollinearity rather than Lasso's ability to perform feature selection, we will now rerun our model more towards Lasso behavior. 

# 4. Lasso Classification 

We chose to perform Elastic Net Classification in order to get a smaller, non-multicollinear subset of injury predictors. With Elastic Net, our optimal alpha leaned mostly towards Ridge, meaning there was limited variable selection. In order to perform variable selection, we will, instead, perform a combination of Lasso Classification and Bootstrapping, denoting which variables appear in most samples. 

We will first start with Lasso Classification without Bootstrapping. 

```{r}
# using our previous training and test sets
cv_lasso <- cv.glmnet(
  x = x_train,
  y = y_train, 
  family = "binomial", 
  alpha = 1, # lasso penalty
  type.measure = "auc", 
  nfolds = 10
)

# plotting lambda results
png("lasso_cv_plot.png", width = 800, height = 600, res = 150)
plot(cv_lasso)
title(main = "Lasso Cross-Validation (α = 1)", line = 2.5)
dev.off()
```

The left-dashed line in the figure above represents the log $/lambda /$ value that produces the highest mean AUC. The right dashed line represents the largest log $/ lamda /$ within one standard error of the minimum, which yields a simpler model with similar performance. We will extract the key lambda values below. 

```{r}
# extracting lambda values
lambda_min <- cv_lasso$lambda.min
lambda_1se <- cv_lasso$lambda.1se
lambda_min; lambda_1se
```

We will explore both the minimum lambda as well as the largest lambda for interpretation purposes. If we are looking for the best predictive AUC to maximize accuracy, we will use lambda min. If our goal is feature selection and interpretability, sacrifing only a minor drop in AUC, we will select lambda 1se. So, we will extract coefficients and nonzero variables for both lambdas.

```{r}
# extracting lambda 1se coefficients and non-zero variables
coef_1se <- coef(cv_lasso, s = "lambda.1se")
nonzero_1se <- rownames(coef_1se)[coef_1se[,1] != 0]

nonzero_1se

# extracting lambda min coefficients and non-zero variables
coef_min <- coef(cv_lasso, s = "lambda.min")
nonzero_min <- rownames(coef_min)[coef_min[,1] != 0]

nonzero_min
```

We see the 1se model resulted in 37 variables, including the intercept while the minimum lambda resulted in 45 variables, including the intercept. We now need to compare model performance between lambda min and lambda 1se to decide whether the sparser model sacrifices any meaninful accuracy compared to the denser model. 

```{r}
# predicting probabilities on the test set
p_min  <- as.numeric(predict(cv_lasso, newx = x_test, s = "lambda.min", type = "response"))
p_1se  <- as.numeric(predict(cv_lasso, newx = x_test, s = "lambda.1se", type = "response"))

# computing ROC and AUC
roc_min <- roc(y_test, p_min, quiet = TRUE)
roc_1se <- roc(y_test, p_1se, quiet = TRUE)

auc_min  <- as.numeric(auc(roc_min))
auc_1se  <- as.numeric(auc(roc_1se))

auc_min; auc_1se

# plotting together
plot(roc_min, col = "#D02130", lwd = 2,
     main = paste0("Test ROC Curves — λ_min vs λ_1se\nAUC(min) = ",
                   round(auc_min, 3), " | AUC(1se) = ", round(auc_1se, 3)))
lines(roc_1se, col = "#1f77b4", lwd = 2)
legend("bottomright", legend = c("λ_min", "λ_1se"),
       col = c("#D02130", "#1f77b4"), lwd = 2, bty = "n")
```

The ROC curves above compare the predictive performance of the lambda min and the lambda 1se Lasso models on the test data. Both achieved nearly identical test AUC values (0.521). An AUC of 0.5 represents random guessing, meaning that the model is no better than chance at distinguishing injured from non-injured jumps. With this, the Lasso model provides very limited discriminative ability on its own, suggesting that the linear relationships among CMJ metrics are not strong predictors of injury risk when considered independently. 

However, because performance was essentially equivalent, the lambda 1se model will most likely be selected for further analysis due to its smaller predictor list. While predictive accuracy was low with just Lasso, this was used to reduce the dimensionality that can be used in non-linear modeling like XGBoost or logistic regression. 


# 5. Fitting Logisitic Regression on our 37 Lasso Selected Variables

We will fit an initial logistic regression using the 37 variable model.

```{r}
# building feature list from cv_lass output with lambda 1se
coef_1se <- coef(cv_lasso, s = "lambda.1se")   
sel_1se <- data.frame(
  Feature     = rownames(coef_1se),
  Coefficient = as.numeric(coef_1se)
)

# keep non-zero and drop intercept
sel_1se <- sel_1se[sel_1se$Coefficient != 0 & sel_1se$Feature != "(Intercept)", ]
selected_vars <- sel_1se$Feature

# making sure all selected features exist in training data
missing_in_train <- setdiff(selected_vars, names(train_df))
if (length(missing_in_train) > 0) {
  message("The following selected features are not columns in train_df and will be dropped: ",
          paste(missing_in_train, collapse = ", "))
  selected_vars <- setdiff(selected_vars, missing_in_train)
}
```


```{r}
# fitting logisic regression on training data
formula_lasso <- as.formula(paste("Injured ~", paste(selected_vars, collapse = " + ")))
logit_model <- glm(formula_lasso, data = train_df, family = binomial(link = "logit"),
                   control = list(maxit = 100))
summary(logit_model)
```


```{r}
# odds ratios and 95% confidence intervals
logit_results <- tidy(logit_model, conf.int = TRUE, exponentiate = TRUE) |>
  dplyr::rename(OR = estimate, CI_low = conf.low, CI_high = conf.high, p_value = p.value) |>
  dplyr::filter(term != "(Intercept)")

# (optional) order by absolute log-OR for readability in plots
logit_results <- logit_results |>
  dplyr::mutate(abs_log_or = abs(log(OR))) |>
  dplyr::arrange(dplyr::desc(abs_log_or))

head(logit_results)
```


```{r}
# keep top N for a compact slide 
N <- min(20, nrow(logit_results))
plot_df <- logit_results[1:N, ]

forest_plot <- ggplot(plot_df, aes(x = reorder(term, OR), y = OR)) +
  geom_hline(yintercept = 1, linetype = "dashed") +
  geom_errorbar(aes(ymin = CI_low, ymax = CI_high), width = 0.2, color = "gray40") +
  geom_point(size = 2.8, color = "#D02130") +
  coord_flip() +
  labs(title = "Logistic Regression on Lasso (λ₁se) Features",
       subtitle = "Odds Ratios with 95% Wald Confidence Intervals",
       x = "CMJ Metric",
       y = "Odds Ratio (logit model)") +
  theme_minimal(base_size = 12) +
  theme(plot.title = element_text(face = "bold"),
        axis.title = element_text(face = "bold"),
        panel.grid.minor = element_blank())

forest_plot
```


# 6. ??? bootstrapping + lasso and get a list of the top contributors to injury risk that appear in most samples