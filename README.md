# DA401 Capstone Project - Jordyn Maurer

## From Jumps to Signals: Selecting Countermovement Jump Features for Injury-Risk Classification
(Write a clear, descriptive title for your project.)

## Research Question
Which force plate metrics can be leveraged to proactively identify athletes at higher risk of injury?

## Data Source
Denison University Strength and Conditioning (force plate data) and Denison University Athletic Training Room (injury data)

## Methods
I. Exploratory Analysis and Feature Selection
	Given the large number of potential predictors, feature selection is essential to avoid overfitting and ensuring the final model highlights the most relevant metrics for injury prediction. Firstly, we will explore logistic regression with Lasso regularization. Lasso regularization applies a penalty to the model’s coefficients, shrinking some to exactly zero and leaving a smaller, more interpretable set of predictors (James et al., 2021, p. 245). This method is particularly useful for our data, where many variables are redundant or highly correlated. To determine the appropriate level of penalty, we will use cross-validation to select the tuning parameter (λ) that minimizes the prediction error (James et al., 2021, p. 250). This analysis will be implemented in R using the glmnet package, specifically designed for fitting Lasso and other penalized regression models (R Core Team, 2025; Friedman et al., 2025). One step further, we will compare Elastic Net (EN) regularization, which blends both Ridge and Lasso penalties. Compared to pure Lasso, EN is often more stable when predictors are correlated and tends to select correlated CMJ metrics together in a "grouping effect,” improving interpretability without sacrificing sparsity (Zou & Hastie, 2005). Implementation will mirror our Lasso setup using the glmnet package, tuning both the mixing parameter (α) and λ using cross-validation grouped by athlete ID. This approach is a well-suited starting point for our study because it manages the high dimensionality, reduces redundancy among correlated variables, and provides an interpretable subset of predictors that can guide subsequent modeling. Together, the results from Lasso and EN will inform which CMJ metrics should be retained for further analysis and guide methods used to examine the relationship among variables. 
	Moreover, we will calculate Variance Inflation Factors (VIFs) to assess multicollinearity among the remaining predictors. The VIF quantifies how much the variance of a coefficient is inflated due to correlation with other predictors, providing a more precise diagnostic than simple pairwise correlations (James et al., 2021, p. 102). Although Lasso and EN penalization mitigate some of this redundancy, computing VIFs offers an additional check to ensure the final subset of CMJ metrics does not include predictors that challenge the interpretability of the model. In addition, we will explore Pearson’s correlation matrix and heatmapping to interpret the relationship and dependence between CMJ metrics. Specifically the squared Pearson correlation coefficient gives an indication on the strength of the linear relationship between two variables (Benesty et al., 2009). When expanded into a matrix of several variables and transformed into a color-coded heat map, we can interpret the relationship between various features in bulk (Khalil et al., 2024). This method offers an alternative feature selection method by identifying highly correlated and redundant variables for potential removal. In doing so, we can better ensure our final model is better suited to identifying the key CMJ metrics associated with soft-tissue injury risk with easier interpretability. 
  
II. Predictive Analysis
	Because soft-tissue injuries are rare events, our dataset is highly imbalanced with more healthy jumps than pre-injury jumps. To address this imbalance, we will implement a bagging approach prior to our predictive analysis. Bagging involves generating multiple bootstrapped samples from the original training set, training a separate model on each sample, and then averaging the predictions, reducing variance and increasing test set accuracy (James et al., 2021). To avoid bias towards the non-injured class, we will train each model on balanced subsets of data that contain equal numbers of injured and non-injured events. For a given test, the final prediction will be calculated as the mean of all model predictions, ensuring both classes are equally represented in the final prediction (Lövdal et al., 2021). Additionally, we will determine the predictors in this analysis using the feature selection methods described above. By first reducing redundancy and narrowing the set of metrics most strongly associated with injury risk, we can more confidently assess the interpretability and efficiency of the final predictive model. 
	For our predictive model, we will follow Lövdal et al. (2021) by employing Extreme Gradient Boosting (XGBoost), a tree-based machine learning algorithm acknowledged for its speed, scalability, and ability to capture complex, non-linear relationships (Chen & Guestrin, 2016). To increase interpretability, we will also use SHAP values to quantify each feature’s direct contribution to the model’s prediction (SAMueL project team, 2022). We will construct the model using the xgboost package in R and obtain SHAP values directly from xgboost’s built-in TreeSHAP function (Chen et al., 2025; Casas, 2019). To prevent information leakage with repeated measures, we will split the data by athlete to make sure jumps from the same athlete do not appear in both training and test sets (Lövdal et al., 2021). With the training data, we will use k-fold cross-validation to tune the model and assess predictive performance. This approach repeatedly partitions the data into k equally sized subsets, training on k-1 folds while validating on the remaining fold, and then averaging the results to estimate the model performance (James et al., 2021, p. 203). After applying our bagging techniques described above, the final XGBoost model will be trained using the optimal parameters identified through cross-validation and applied to the test set. 
We will also get SHAP values for each k-fold split which will be reported as log odds shifts in model prediction. We will then find summary statistics of the SHAP values to identify the CMJ metrics that consistently have the largest impact on injury prediction. We will provide a beeswarm plot to display the mean absolute SHAP values, providing a ranked overview of which metrics are most influential (SAMueL project team, 2022). By comparing SHAP rankings with the features selected through Lasso and correlation filtering, we can identify the most important variables and improve interpretability of the model. To assess the model’s performance, we will use a receiving operator characteristic (ROC) curve to describe the fraction of true positives versus the corresponding rate of false positives (Lövdal et al., 2021). The associated area under the curve (AUC) describes the performance of the model with a value closer to 1 representing a better predictive model (Lövdal et al., 2021). In addition, we will report precision and the F1 score to further evaluate classification performance under class imbalance. Precision measures the proportion of predicted injury cases that are correctly identified. Sensitivity, on the other hand, measures the proportion of actual injury cases that are correctly detected. The F1 score represents the harmonic mean of precision and sensitivity, providing a single metric that balances both false positives and false negatives (Powers, 2011). Because our dataset contains far fewer injury events than healthy ones, these measures provide a more meaningful evaluation of accuracy than overall classification rate when assessing performance on the injured class. Through a combination of these statistical methods, we hope to identify the most predictive CMJ metrics for soft-tissue injury risk while remaining conscious of the model’s predictive power, context, and accuracy.

## Expected Timeline
<img width="1036" height="341" alt="image" src="https://github.com/user-attachments/assets/19c59ada-046f-4811-bccc-49eb65b01902" />


## Google Drive 
(https://drive.google.com/drive/folders/1aTM14L0AzK1xiSqmDgkvh7ZZ9AcbEZ7o?usp=drive_link)

## Repository Structure
- `code/`: All analysis scripts or notebooks
- `writing/`: Proposal, drafts, final paper
- `figures/`: Plots, charts, visualizations
- `data/`: Only small sample data (large files go in Google Drive)
