---
title: "Feature_Selection"
output: html_document
author: "Jordyn Maurer"
date: "2025-10-13"
---


The following code seeks to decrease the number of injury predictors using Elastic Net and Lasso regularization. To begin, the file explores correlations among different CMJ variables, identifies a need for re-weighting the non-injured class, completes elastic net, moves to lasso regularization, and writes the final set of predictors to a .csv ("lasso_selected_43vars.csv") for use in the XGBoosting model. This .csv can be found in the Google Drive and github repository for reference. 

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
set.seed(42) # ensures reproducibility across all chunks
library(readr)
library(dplyr)
library(corrplot)
library(reshape2)
library(ggplot2)
library(ggcorrplot)
library(glmnet)
library(caret)
library(pROC)
library(broom)
library(car)
library(stringr)
library(forcats)
library(scales)
library(gt)
```



# 1. Loading and Inspecting Data
```{r}
# setting directory (probably needs to be changed later)
getwd()
setwd("C:\\Users\\jordy\\OneDrive\\Documents\\Coach_Scott\\Injuries") # input personal project directory here
data <- read.csv("final_data.csv")
```

```{r}
# checking data structure
str(data)

summary(data$Injured)

# making sure Injured is numeric
data$Injured <- as.numeric(data$Injured)
```

Now, we need to remove identifiers and non-numeric colummns. These identifiers include IDs, organizations, dates, and injury types. We will remove all character variables as well as "days_before" which was a helper variable to identify injured vs. non-injured jumps.  

```{r}
# removing identifiable information that will not be involved in the study and keeping only numeric data.  
data_numeric <- data %>%
  select(where(~ !is.character(.))) %>%
  select(-days_before)

str(data_numeric)
```

# 2. Exploring Correlations Among CMJ Metrics

```{r}
# initial plot with all metrics
corr_matrix <- cor(data_numeric %>% select(-Injured), use = "pairwise.complete.obs")
corrplot(corr_matrix, method = "color", tl.cex = 0.5)
```

From the above heatmap, we see there are an overwhelming number of metrics to compare all at once. Instead of plotting them all at once, we will instead find only the metrics with a relation greater than 0.8. 

```{r}
# filtering values above 0.8
corr_long <- melt(corr_matrix) %>%
  filter(Var1 != Var2, abs(value) > 0.8) %>%  
  arrange(desc(abs(value)))

corr_long # viewing

# seeing how many unique variables are in this output
unique_vars <- unique(c(corr_long$Var1, corr_long$Var2))
length(unique_vars)
```

Above, we've created a table of variable pairs that are highly correlated. We see that there are 70 unique variables correlated with each other. This is what we expected from the CMJ in which each movement relates to the next, showing lots of correlated metrics. We will now create a histogram to demonstrate the spread of correlation values. 

```{r}
# creating histogram of correlations
corr_values <- melt(corr_matrix)$value

corr_coeff <- ggplot(data.frame(corr_values), aes(x = corr_values)) +
  geom_histogram(bins = 50, fill = "#D02130", color = "white") +
  labs(
    title = "Distribution of Pairwise Correlations Among CMJ Metrics",
    x = "Correlation Coefficient (r)",
    y = "Count"
  ) +
  theme_minimal()
corr_coeff

ggsave("correlation_coefficients_his.png", plot = corr_coeff, width = 6, height = 4, units = "in", dpi = 300)
```

The above visual demonstrates that many CMJ metrics are strongly interrelated, while others remain only weakly associated. The clustering of high positive correlations reflects the biomechanicial origins of the derivation of the CMJ metrics corresponding with particular phases. This widespread multicollinearity highlights the need for penalized regression methods that can manage correlated predictors and isolate those that provide unique information about injury risk. 

# 3. Setting Up the Training/Testing Sets

We will now use Elastic Net that blends lasso ($\alpha = 1$) and ridge ($\alpha = 0$) methods, accounting for multicollinearity and variable selection. 

```{r}
# performing an 80/20 split by athlete ID for train and test 
ids <- unique(data$ID)
train_ids <- sample(ids, size = floor(0.80 * length(ids)))
is_train <- data$ID %in% train_ids

train_df <- data_numeric[is_train, ]
test_df <- data_numeric[!is_train, ]
```

Before running elastic net, we are going to check for missing values that may affect the process. 

```{r}
# counting NA values per column in training set
na_counts <- colSums(is.na(train_df))

# displaying only columns that actually have missing values
na_counts[na_counts > 0]

# checking NA percentages
na_percent <- (na_counts / nrow(train_df)) * 100
na_percent[na_percent > 0]  # only show columns with missing data

```

From the above output, we see there are 12 columns with 0.16% missing values. To account for these, we will insert the median of the column wherever the missing value occurs. 

```{r}
# median imputation

# identifying numeric columns (excluding Injured)
num_cols <- setdiff(names(train_df), "Injured")

# computing medians from the training data
meds <- sapply(train_df[, num_cols, drop = FALSE],
               function(x) median(x, na.rm = TRUE))

# replacing NA values in the training and test sets with those medians
for (nm in num_cols) {
  # Impute in training data
  train_df[[nm]][is.na(train_df[[nm]])] <- meds[[nm]]
  
  # Impute in test data (using training medians!)
  if (nm %in% names(test_df)) {
    test_df[[nm]][is.na(test_df[[nm]])] <- meds[[nm]]
  }
}

# checking to see that it worked (should return zero for both)
sum(is.na(train_df))
sum(is.na(test_df))
```

We now have removed any NA values from the training and test sets that may influence our Elastic Net Classification. We will now create the model matrices and scale our predictors.

```{r}
# creating predictor matrix (x) and outcome vector (y)
x_train <- model.matrix(Injured ~ ., data = train_df)[, -1]
y_train <- train_df$Injured

x_test  <- model.matrix(Injured ~ ., data = test_df)[, -1]
y_test  <- test_df$Injured
```


# 4. Elastic Net (Weighted)

We'll begin by visualizing the proportion of Injured vs. Non-Injured jumps. 

```{r}
# counts & proportions
injury_counts <- data_numeric %>%
  dplyr::count(Injured) %>%
  dplyr::mutate(
    Class = factor(ifelse(Injured == 1, "Injured", "Non-Injured"),
                   levels = c("Non-Injured", "Injured")),
    Proportion = n / sum(n),
    Label = paste0(round(Proportion * 100, 1), "%")
  )

# % injured for the title
injured_pct <- round(injury_counts$Proportion[injury_counts$Class == "Injured"] * 100, 1)

fig1 <- ggplot(injury_counts, aes(x = Class, y = Proportion, fill = Class)) +
  geom_col(width = 0.6) +
  scale_fill_manual(values = c("Non-Injured" = "#D02130", "Injured" = "gray80")) +
  scale_y_continuous(
    labels = scales::percent_format(accuracy = 1),
    limits = c(0, 1),
    expand = expansion(mult = c(0, 0.08))
  ) +
  labs(
    title = paste0("Figure 1. Only ", injured_pct, "% of Jumps Were Injured Events"),
    subtitle = "Distribution of injury status across all jumps",
    x = NULL,
    y = "Proportion of Total Jumps"
  ) +
  theme_minimal(base_size = 12) +
  theme(
    plot.title = element_text(face = "bold", size = 12),
    plot.subtitle = element_text(size = 10, color = "gray30"),
    axis.title = element_text(face = "bold", size = 9),
    legend.position = "none",
    panel.grid.minor = element_blank(),
    panel.grid.major.x = element_blank()
  )

fig1


fig1

ggsave("figure1_injury_proportion.png", plot = fig1, width = 6, height = 4, dpi = 300)
```


```{r}
# ensure binary 0/1
y_train <- ifelse(y_train == 1, 1, 0)
y_test  <- ifelse(y_test  == 1, 1, 0)

# class-balanced weights for training rows only
n1 <- sum(y_train == 1)
n0 <- sum(y_train == 0) # counting how many observations belong to each class
wts_train <- ifelse(y_train == 1, 0.5 / n1, 0.5 / n0) # the total weight of all non injured and injured rows equals 0.5, allowing glmnet to treat both classes equally

# quick check
table(y_train) # confirms class counts
summary(wts_train) # confirms weight distribution
```

```{r}
# lock folds so results don't change
K <- 10
set.seed(42)  
foldid <- sample(rep(1:K, length.out = length(y_train)))

# alpha sweep (no caching)
alphas_w <- seq(0, 1, by = 0.1)
cv_results_w <- data.frame(alpha = alphas_w, mean_AUC = NA_real_)

for (i in seq_along(alphas_w)) {
  cv_w <- cv.glmnet(
    x = x_train, y = y_train,
    family = "binomial",
    alpha = alphas_w[i],
    type.measure = "auc",
    nfolds = K,
    weights = wts_train,
    foldid = foldid,     
    parallel = FALSE
  )
  cv_results_w$mean_AUC[i] <- max(cv_w$cvm)
}

best_alpha_w <- cv_results_w$alpha[which.max(cv_results_w$mean_AUC)]
best_alpha_w
cv_results_w
```

After weighting the training set, we received a different alpha of 0.2, but it still leans towards Ridge rather than Lasso, so we'll have limited feature selection.  

```{r}
# Plot mean AUC across alpha values (Weighted Elastic Net)
best_idx_w <- which.max(cv_results_w$mean_AUC)
best_alpha_w <- cv_results_w$alpha[best_idx_w]
best_auc_w <- cv_results_w$mean_AUC[best_idx_w]

alpha_plot_w <- ggplot(cv_results_w, aes(x = alpha, y = mean_AUC)) +
  geom_line(color = "black", linewidth = 1) +
  geom_point(size = 3, color = "black") +
  geom_vline(xintercept = best_alpha_w, linetype = "dashed", color = "#D02130", linewidth = 0.8) +
  labs(
    title = str_wrap("Figure 2: Weighted elastic net regularization yielded an alpha penalty mix most reflective of Ridge regularization", width = 70),
    subtitle = "Class-weighted 10-fold CV balancing Injured (6%) and Non-Injured (94%) jumps",
    x = expression(alpha ~ "(Penalty Mix: Ridge → Lasso)"),
    y = "Mean AUC (10-Fold CV, Weighted)"
  ) +
  theme_minimal(base_size = 12) +
  theme(
    plot.title = element_text(face = "bold", size = 11.5),
    plot.subtitle = element_text(size = 10, color = "gray30"),
    axis.title = element_text(face = "bold", size = 9),
    panel.grid.minor = element_blank(),
    panel.grid.major.x = element_blank()
  )

alpha_plot_w


ggsave("fig2_weighted_alpha_values.png", plot = alpha_plot_w, width = 6, height = 4, units = "in", dpi = 300)
```


```{r}
# run elastic net with that alpha (still using class weights)
cv_elnet_w <- cv.glmnet(
  x = x_train, 
  y = y_train,
  family = "binomial",
  alpha = best_alpha_w,
  type.measure = "auc",
  nfolds = 10,
  weights = wts_train
)

# plot the cross-validation results
png("elasticnet_plot_weighted.png", width = 800, height = 600, res = 150)

plot(cv_elnet_w)
title(main = paste("Figure 3. Elastic Net CV (Weighted, α =", best_alpha_w, ")"), line = 2.5)
```


```{r}
# extracting the best lambda (penalty)
best_lambda_w <- cv_elnet_w$lambda.min
best_lambda_w

# view coefficients at best alpha/lambda
coef_elnet_w <- coef(cv_elnet_w, s = "lambda.min")
coef_elnet_w
```

Because our alpha value stayed closely to 0, this was practically Ridge. So, while it handled multicollinearity of our variables, it did not shrink coefficients to zero, meaning there was no variable selection. We will now do Lasso with our weighted classes. 

# 5. Lasso (Weighted)

```{r}
# cross-validated Lasso with class weights
set.seed(42)
K <- 10
foldid <- sample(rep(1:K, length.out = length(y_train)))

cv_lasso_w <- cv.glmnet(
  x = x_train, y = y_train,
  family = "binomial", alpha = 1,
  type.measure = "auc", nfolds = K,
  weights = wts_train, foldid = foldid
)

```


```{r}
# plotting CV curve
png("lasso_cv_plot_weighted.png", width = 800, height = 600, res = 150)
plot(cv_lasso_w)
title(main = "Figure 4. Lasso Cross-Validation (Weighted, α = 1)", line = 2.5)

# Extract key lambdas
lambda_min_w <- cv_lasso_w$lambda.min
lambda_1se_w <- cv_lasso_w$lambda.1se
lambda_min_w; lambda_1se_w

```

Lambda min of 0.00022 yielded the highest AUC during training, and lambda 1se = 0.00047 represents a simpler model within one standard error of the minimum. The cross-validation curve shows that the model performance steadily increase as lambda decreased, peaking near 0.64. The gap between lambda min and lambda 1se is small, indicating that the simpler lambda 1se model sacrifices minimal predictive performance while retaining fewer predictors. 

```{r}
# Coefficients & selected features at λ_min and λ_1se (weighted)
coef_min_w <- coef(cv_lasso_w, s = "lambda.min")
coef_1se_w <- coef(cv_lasso_w, s = "lambda.1se")

nonzero_min_w <- rownames(coef_min_w)[coef_min_w[, 1] != 0]
nonzero_1se_w <- rownames(coef_1se_w)[coef_1se_w[, 1] != 0]

nonzero_1se_w     # inspect
length(nonzero_1se_w); length(nonzero_min_w)
```

From the above output, we see our lambda 1se model yielded 43 nonzero predictors, a meaningful reduction from the 77 predictors retained by Elastic Net. 

```{r}
# test-set performance: λ_min vs λ_1se (weighted)
p_min_w <- as.numeric(predict(cv_lasso_w, newx = x_test, s = "lambda.min", type = "response"))
p_1se_w <- as.numeric(predict(cv_lasso_w, newx = x_test, s = "lambda.1se", type = "response"))

roc_min_w <- roc(y_test, p_min_w, quiet = TRUE)
roc_1se_w <- roc(y_test, p_1se_w, quiet = TRUE)

auc_min_w <- as.numeric(auc(roc_min_w))
auc_1se_w <- as.numeric(auc(roc_1se_w))
auc_min_w; auc_1se_w

# Plot both weighted ROC curves
png("weighted_lasso_roc.png", width = 800, height = 600, res = 150)
plot(roc_min_w, col = "#D02130", lwd = 2,
     main = paste0("Figure 5. Test ROC — Weighted Lasso: λ_min vs λ_1se\n",
                   "AUC(min) = ", round(auc_min_w, 3),
                   " | AUC(1se) = ", round(auc_1se_w, 3)))
lines(roc_1se_w, col = "#1f77b4", lwd = 2)
legend("bottomright", legend = c("λ_min (weighted)", "λ_1se (weighted)"),
       col = c("#D02130", "#1f77b4"), lwd = 2, bty = "n")

```


The above graph shows the discriminatory ability of both the lambda min and lambda 1se model, showing how well they separate injured from non-injured jumps. The ROC curve plots sensitivity (true positive rate) vs. specificity (false positive rate). The AUC is a summary measure with 0.5 meaning random guessing and 1 being perfect discrimination. Because both lambda min and lambda 1se were only slightly above 0.5, this means our models struggle to separate the two groups on the test set. Similarly, looking at the ROC curves, we see both the red and the blue line hug diagonal, meaning the model's predictions are only slightly better than random. 

This altogether implies that injury risk is not well explained by a purely linear combination of these CMJ metrics. 

We will now compute the Variance Inflation Factor on the remaining 43 variables. 

```{r}
lasso_vars_w <- setdiff(nonzero_1se_w, "(Intercept)")
X <- train_df[, lasso_vars_w, drop = FALSE]

# ensuring no missing values
stopifnot(sum(is.na(X)) == 0)

# drop near-zero variance predictors (we had aliasing before so include this to get rid of that)
nzv_idx <- nearZeroVar(X)
if (length(nzv_idx) > 0) {
  X <- X[, -nzv_idx, drop = FALSE]
}

# remove perfect linear combinations (exact multicollinearity since we had this error before)
combos <- findLinearCombos(X)  # finds columns participating in linear combos
if (!is.null(combos$remove) && length(combos$remove) > 0) {
  X <- X[, -combos$remove, drop = FALSE]
}

# fit a simple linear model (only to compute VIFs)
dat_vif <- cbind(Injured = as.numeric(train_df$Injured), X)
fit_vif <- lm(Injured ~ ., data = dat_vif)

# compute VIFs
vifs <- car::vif(fit_vif)            
vifs_sorted <- sort(vifs, decreasing = TRUE)

# peek and summarize
head(vifs_sorted, 10)
summary(vifs)
length(vifs)
```

High multicollinearity persists among Lasso-selected CMJ metrics, yet, this still performed feature selection under penalized regularization. This means it removed noisy, redundant, and collinear predictors from the original 70+ CMJ metrics. The 43 variables it retained are those with the strongest linear association to injury. If we apply this subset to something like XGBoosting to perform the prediction, this model does not require linearity and, instead, captures non-linear interactions automatically. We can apply this subset as the starting point for our future predictive model to avoid overfitting on noise from the original dataset, shrink training time and improve interpretability, and retain variables with proved importance. 

We will now create a new dataset with the 43 selected variables to put into our XGBoosting Model. 

# 6. Writing a New .csv With Lasso (Weighted) Selected Variables

```{r}
# extracting variable names (without the intercept)
lasso_vars_final <- setdiff(nonzero_1se_w, "(Intercept)")

# subset from the full original data_numeric to keep 'Injured' and lasso variables
xgb_data <- data_numeric %>%
dplyr::select(Injured, all_of(lasso_vars_final))

# confirm dimensions and structure
dim(xgb_data)
str(xgb_data)
names(xgb_data)
```

```{r}
# saving to a .csv
write.csv(xgb_data, "lasso_selected_43vars.csv", row.names = FALSE)
```

