---
title: "Feature_Selection"
output: html_document
date: "2025-10-13"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
set.seed(42) # ensures reproducibility across all chunks
library(readr)
library(dplyr)
library(corrplot)
library(reshape2)
library(ggplot2)
library(ggcorrplot)
library(glmnet)
library(caret)
library(pROC)
library(broom)
```



# 1. Loading and Inspecting Data
```{r}
# setting directory (probably needs to be changed later)
getwd()
setwd("C:\\Users\\jordy\\OneDrive\\Documents\\Coach_Scott\\Injuries")
data <- read.csv("final_data.csv")
```

```{r}
# checking data structure
str(data)

summary(data$Injured)

# making sure INjured is numeric
data$Injured <- as.numeric(data$Injured)
```

Now, we need to remove identifiers and non-numeric colummns. These identifiers include IDs, organizations, dates, and injury types.We will remove all character variables as well as "days_before" which was a helper variable to identify injured vs. non-injured jumps.  

```{r}
# removing identifiable information that will not be involved in the study and keeping only numeric data.  
data_numeric <- data %>%
  select(where(~ !is.character(.))) %>%
  select(-days_before)

str(data_numeric)
```

# 2. Exploring Correlations Among CMJ Metrics

```{r}
# initial plot with all metrics
corr_matrix <- cor(data_numeric %>% select(-Injured), use = "pairwise.complete.obs")
corrplot(corr_matrix, method = "color", tl.cex = 0.5)
```

From the above heatmap, we see there are an overwhelming number of metrics to compare all at once. Instead of plotting them all at once, we will instead find only the metrics with a relation greater than 0.8. 

```{r}
# filtering values above 0.8
corr_long <- melt(corr_matrix) %>%
  filter(Var1 != Var2, abs(value) > 0.8) %>%  
  arrange(desc(abs(value)))

corr_long # viewing

# seeing how many unique variables are in this output
unique_vars <- unique(c(corr_long$Var1, corr_long$Var2))
length(unique_vars)
```

Above, we've created a table of variable pairs that are highly correlated. We see that there are 70 unique variables correlated with each other. This is what we expected from the CMJ in which each movement relates to the next, showing lots of correlated metrics. We will now create a histogram to demonstrate the spread of correlation values. 

```{r}
# creating histogram of correlations
corr_values <- melt(corr_matrix)$value

corr_coeff <- ggplot(data.frame(corr_values), aes(x = corr_values)) +
  geom_histogram(bins = 50, fill = "#D02130", color = "white") +
  labs(
    title = "Distribution of Pairwise Correlations Among CMJ Metrics",
    x = "Correlation Coefficient (r)",
    y = "Count"
  ) +
  theme_minimal()
corr_coeff

ggsave("correlation_coefficients_his.png", plot = corr_coeff, width = 6, height = 4, units = "in", dpi = 300)
```

The above visual demonstrates that many CMJ metrics are strongly interrelated, while others remain only weakly associated. The clustering of high positive correlations reflects the biomechanicial origins of the derivation of the CMJ metrics corresponding with particular phases. This widespread multicollinearity highlights the need for penalized regression methods that can manage correlated predictors and isolate those that provide unique information about injury risk. 

# 3. Elastic Net Classification for Feature Selection

We will now use Elastic Net that blends lasso ($\alpha = 1$) and ridge ($\alpha = 0$) methods, accounting for multicollinearity and variable selection. 

```{r}
# performing an 80/20 split by athlete ID for train and test 
ids <- unique(data$ID)
train_ids <- sample(ids, size = floor(0.80 * length(ids)))
is_train <- data$ID %in% train_ids

train_df <- data_numeric[is_train, ]
test_df <- data_numeric[!is_train, ]
```

Before running elastic net, we are going to check for missing values that may affect the process. 

```{r}
# counting NA values per column in training set
na_counts <- colSums(is.na(train_df))

# displaying only columns that actually have missing values
na_counts[na_counts > 0]

# checking NA percentages
na_percent <- (na_counts / nrow(train_df)) * 100
na_percent[na_percent > 0]  # only show columns with missing data

```

From the above output, we see there are 12 columns with 0.16% missing values. To account for these, we will insert the median of the column wherever the missing value occurs. 

```{r}
# median imputation

# identifying numeric columns (excluding Injured)
num_cols <- setdiff(names(train_df), "Injured")

# computing medians from the training data
meds <- sapply(train_df[, num_cols, drop = FALSE],
               function(x) median(x, na.rm = TRUE))

# replacing NA values in the training and test sets with those medians
for (nm in num_cols) {
  # Impute in training data
  train_df[[nm]][is.na(train_df[[nm]])] <- meds[[nm]]
  
  # Impute in test data (using training medians!)
  if (nm %in% names(test_df)) {
    test_df[[nm]][is.na(test_df[[nm]])] <- meds[[nm]]
  }
}

# checking to see that it worked (should return zero for both)
sum(is.na(train_df))
sum(is.na(test_df))
```

We now have removed any NA values from the training and test sets that may influence our Elastic Net Classification. We will now create the model matrices and scale our predictors.

```{r}
# creating predictor matrix (x) and outcome vector (y)
x_train <- model.matrix(Injured ~ ., data = train_df)[, -1]
y_train <- train_df$Injured

x_test  <- model.matrix(Injured ~ ., data = test_df)[, -1]
y_test  <- test_df$Injured
```

Next, we'll search for the optimal value of $\alpha$ with 10-fold cross-validation, using AUC as our judge of model performance.

## 3.1 Finding the optimal alpha ($\alpha$) for Elastic Net

```{r}
# defining a range of alpha values from ridge (0) to lasso (1)
alphas <- seq(0, 1, by = 0.1)

cv_results <- data.frame(alpha = alphas, mean_AUC = NA)

for (i in seq_along(alphas)) {
  cv <- cv.glmnet(
    x_train, y_train,
    family = "binomial",
    alpha = alphas[i],
    type.measure = "auc", 
    nfolds = 10,
    parallel = TRUE
  )
  cv_results$mean_AUC[i] <- max(cv$cvm)
}

cv_results
```

We will plot the results for readability. 

```{r}
best_idx <- which.max(cv_results$mean_AUC)
best_alpha <- cv_results$alpha[best_idx]
best_auc <- cv_results$mean_AUC[best_idx]

alpha_plot <- ggplot(cv_results, aes(x = alpha, y = mean_AUC)) +
  geom_line(color = "#1f77b4", linewidth = 1) +
  geom_point(size = 3, color = "#1f77b4") +
  geom_vline(xintercept = best_alpha, linetype = "dashed", color = "red", linewidth = 0.8) +
  annotate(
    "text",
    x = best_alpha + 0.1,   # move text to the right
    y = best_auc + 0.002,   # move text slightly above line
    label = paste0("Best α = ", round(best_alpha, 2), "\nAUC = ", round(best_auc, 3)),
    color = "red",
    size = 3,
    hjust = 0
  ) +
  labs(
    title = "Mean AUC Across Alpha Values in Elastic Net Cross-Validation",
    subtitle = "Evaluating Elastic Net balance between Ridge (α = 0) and Lasso (α = 1)",
    x = expression(alpha ~ "(Penalty Mix: Ridge → Lasso)"),
    y = "Mean AUC (10-Fold CV)"
  ) +
  theme_minimal(base_size = 12) +
  theme(
    plot.title = element_text(face = "bold", size = 12),
    plot.subtitle = element_text(size = 10, color = "gray30"),
    axis.title = element_text(face = "bold", size = 9),
    panel.grid.minor = element_blank(),
    panel.grid.major.x = element_blank()
  )
alpha_plot

ggsave("optimal_alpha_values.png", plot = alpha_plot, width = 6, height = 4, units = "in", dpi = 300)
```

From the above output, we are shown how the mean AUC varies across diferent values of the mixing parameter, $/alpha $. Model performance peaked at $/alpha = 0.1$ (AUC = 0.646), indicating a structure more similar to Ridge regression. This makes sense considering many of the variables are highly correlated. 

```{r}
# creating a variable for the optimal alpha value
best_alpha <- cv_results$alpha[which.max(cv_results$mean_AUC)]
best_alpha
```

Since we have found the optimal $/alpha$ value, we will now find the best $/lambda$ (penalty) using 10-fold cross-validation. 

## 3.2 Fitting Elastic Net with the Optimal Alpha

```{r}
# finding the best lambda
cv_elnet <- cv.glmnet(
  x_train, y_train,
  family = "binomial",
  alpha = best_alpha,
  type.measure = "auc",
  nfolds = 10
)

png("elasticnet_plot.png", width = 800, height = 600, res = 150)
plot(cv_elnet)
title(main = paste("Elastic Net Cross-Validation (α =", best_alpha, ")"), line = 2.5)
```

The figure above shows the results of 10-fold cross-validation used to determinal the optimal penalty parameter for the model at alpha = 0.1. Each red point represents the mean AUC for a given lambda value, while the grey bars indicate the standard errors. The left-dashed line represents the value that produces the highest average AUC. The right-dashed line is the largest lambda within one standard error of the minimum, which provides a similar performance with fewer predictors. We will go forward with the minimum lambda value. 

We will now extract this lamdba value and get coefficients for the best alpha/lambda combination. 

```{r}
# getting best lambda
best_lambda <- cv_elnet$lambda.min
best_lambda

# getting coefficients for the best alpha/lambda combo
coef_elnet <- coef(cv_elnet, s = "lambda.min")
coef_elnet
```

Because our alpha value leaned more towards Ridge, there was limited feature selection that took place after using the minimum lambda value. It shrinks the coefficients but does not zero them out. However, we want a smaller set of predictors to feed into a predictive model. Since $/alpha = 0.1$ was too similar to Ridge regression in handling multicollinearity rather than Lasso's ability to perform feature selection, we will now rerun our model more towards Lasso behavior. 

# 4. Lasso Classification 

We chose to perform Elastic Net Classification in order to get a smaller, non-multicollinear subset of injury predictors. With Elastic Net, our optimal alpha leaned mostly towards Ridge, meaning there was limited variable selection. In order to perform variable selection, we will, instead, perform a combination of Lasso Classification and Bootstrapping, denoting which variables appear in most samples. 

We will first start with Lasso Classification without Bootstrapping. 

```{r}
# using our previous training and test sets
cv_lasso <- cv.glmnet(
  x = x_train,
  y = y_train, 
  family = "binomial", 
  alpha = 1, # lasso penalty
  type.measure = "auc", 
  nfolds = 10
)

# plotting lambda results
png("lasso_cv_plot.png", width = 800, height = 600, res = 150)
plot(cv_lasso)
title(main = "Lasso Cross-Validation (α = 1)", line = 2.5)
```

The left-dashed line in the figure above represents the log $/lambda /$ value that produces the highest mean AUC. The right dashed line represents the largest log $/ lamda /$ within one standard error of the minimum, which yields a simpler model with similar performance. We will extract the key lambda values below. 

```{r}
# extracting lambda values
lambda_min <- cv_lasso$lambda.min
lambda_1se <- cv_lasso$lambda.1se
lambda_min; lambda_1se
```

We will explore both the minimum lambda as well as the largest lambda for interpretation purposes. If we are looking for the best predictive AUC to maximize accuracy, we will use lambda min. If our goal is feature selection and interpretability, sacrifing only a minor drop in AUC, we will select lambda 1se. So, we will extract coefficients and nonzero variables for both lambdas.

```{r}
# extracting lambda 1se coefficients and non-zero variables
coef_1se <- coef(cv_lasso, s = "lambda.1se")
nonzero_1se <- rownames(coef_1se)[coef_1se[,1] != 0]

nonzero_1se

# extracting lambda min coefficients and non-zero variables
coef_min <- coef(cv_lasso, s = "lambda.min")
nonzero_min <- rownames(coef_min)[coef_min[,1] != 0]

nonzero_min
```

We see the 1se model resulted in 37 variables, including the intercept while the minimum lambda resulted in 45 variables, including the intercept. We now need to compare model performance between lambda min and lambda 1se to decide whether the sparser model sacrifices any meaninful accuracy compared to the denser model. 

```{r}
# predicting probabilities on the test set
p_min  <- as.numeric(predict(cv_lasso, newx = x_test, s = "lambda.min", type = "response"))
p_1se  <- as.numeric(predict(cv_lasso, newx = x_test, s = "lambda.1se", type = "response"))

# computing ROC and AUC
roc_min <- roc(y_test, p_min, quiet = TRUE)
roc_1se <- roc(y_test, p_1se, quiet = TRUE)

auc_min  <- as.numeric(auc(roc_min))
auc_1se  <- as.numeric(auc(roc_1se))

auc_min; auc_1se

# plotting together
plot(roc_min, col = "#D02130", lwd = 2,
     main = paste0("Test ROC Curves — λ_min vs λ_1se\nAUC(min) = ",
                   round(auc_min, 3), " | AUC(1se) = ", round(auc_1se, 3)))
lines(roc_1se, col = "#1f77b4", lwd = 2)
legend("bottomright", legend = c("λ_min", "λ_1se"),
       col = c("#D02130", "#1f77b4"), lwd = 2, bty = "n")
```

The ROC curves above compare the predictive performance of the lambda min and the lambda 1se Lasso models on the test data. Both achieved nearly identical test AUC values (0.521). An AUC of 0.5 represents random guessing, meaning that the model is no better than chance at distinguishing injured from non-injured jumps. With this, the Lasso model provides very limited discriminative ability on its own, suggesting that the linear relationships among CMJ metrics are not strong predictors of injury risk when considered independently. 

However, because performance was essentially equivalent, the lambda 1se model will most likely be selected for further analysis due to its smaller predictor list. While predictive accuracy was low with just Lasso, this was used to reduce the dimensionality that can be used in non-linear modeling like XGBoost or logistic regression. 


# 5. Fitting Logisitic Regression on our 37 Lasso Selected Variables

We will fit an initial logistic regression using the 37 variable model.

```{r}
# building feature list from cv_lass output with lambda 1se
coef_1se <- coef(cv_lasso, s = "lambda.1se")   
sel_1se <- data.frame(
  Feature     = rownames(coef_1se),
  Coefficient = as.numeric(coef_1se)
)

# keep non-zero and drop intercept
sel_1se <- sel_1se[sel_1se$Coefficient != 0 & sel_1se$Feature != "(Intercept)", ]
selected_vars <- sel_1se$Feature

# making sure all selected features exist in training data
missing_in_train <- setdiff(selected_vars, names(train_df))
if (length(missing_in_train) > 0) {
  message("The following selected features are not columns in train_df and will be dropped: ",
          paste(missing_in_train, collapse = ", "))
  selected_vars <- setdiff(selected_vars, missing_in_train)
}
```


```{r}
# fitting logisic regression on training data
formula_lasso <- as.formula(paste("Injured ~", paste(selected_vars, collapse = " + ")))
logit_model <- glm(formula_lasso, data = train_df, family = binomial(link = "logit"),
                   control = list(maxit = 100))
summary(logit_model)
```


```{r}
# odds ratios and 95% confidence intervals
logit_results <- tidy(logit_model, conf.int = TRUE, exponentiate = TRUE) |>
  dplyr::rename(OR = estimate, CI_low = conf.low, CI_high = conf.high, p_value = p.value) |>
  dplyr::filter(term != "(Intercept)")

# (optional) order by absolute log-OR for readability in plots
logit_results <- logit_results |>
  dplyr::mutate(abs_log_or = abs(log(OR))) |>
  dplyr::arrange(dplyr::desc(abs_log_or))

head(logit_results)
```


# 6. Elastic Net (Weighted)

```{r}
# ensure binary 0/1
y_train <- ifelse(y_train == 1, 1, 0)
y_test  <- ifelse(y_test  == 1, 1, 0)

# class-balanced weights for TRAIN rows only
n1 <- sum(y_train == 1)
n0 <- sum(y_train == 0) # counting how many observations belong to each class
wts_train <- ifelse(y_train == 1, 0.5 / n1, 0.5 / n0) # the total weight of all non injured and injured rows equals 0.5, allowing glmnet to treat both classes equally

# quick check
table(y_train) # confirms class counts
summary(wts_train) # confirms weight distribution
```

```{r}
alphas_w <- seq(0, 1, by = 0.1) # creating alpha values
cv_results_w <- data.frame(alpha = alphas_w, mean_AUC = NA_real_)

for (i in seq_along(alphas_w)) {
  cv_w <- cv.glmnet(
    x = x_train, y = y_train,
    family = "binomial",
    alpha = alphas_w[i],
    type.measure = "auc",
    nfolds = 10,
    weights = wts_train
  )
  cv_results_w$mean_AUC[i] <- max(cv_w$cvm)
}

cv_results_w
best_alpha_w <- cv_results_w$alpha[which.max(cv_results_w$mean_AUC)]
best_alpha_w
```

After weighting the training set, we received a different alpha of 0.2, but it still leans towards Ridge rather than Lasso, so we'll have limited feature selection.  

```{r}
# Plot mean AUC across alpha values (Weighted Elastic Net)
best_idx_w <- which.max(cv_results_w$mean_AUC)
best_alpha_w <- cv_results_w$alpha[best_idx_w]
best_auc_w <- cv_results_w$mean_AUC[best_idx_w]

alpha_plot_w <- ggplot(cv_results_w, aes(x = alpha, y = mean_AUC)) +
  geom_line(color = "#1f77b4", linewidth = 1) +
  geom_point(size = 3, color = "#1f77b4") +
  geom_vline(xintercept = best_alpha_w, linetype = "dashed", color = "red", linewidth = 0.8) +
  annotate(
    "text",
    x = best_alpha_w + 0.1,   # move text to the right of the dashed line
    y = best_auc_w + 0.002,   # move text slightly above
    label = paste0("Best α = ", round(best_alpha_w, 2), "\nAUC = ", round(best_auc_w, 3)),
    color = "red",
    size = 3,
    hjust = 0
  ) +
  labs(
    title = "Mean AUC Across Alpha Values (Weighted Elastic Net)",
    subtitle = "Class-weighted 10-fold CV balancing Injured (6%) and Non-Injured (94%) jumps",
    x = expression(alpha ~ "(Penalty Mix: Ridge → Lasso)"),
    y = "Mean AUC (10-Fold CV, Weighted)"
  ) +
  theme_minimal(base_size = 12) +
  theme(
    plot.title = element_text(face = "bold", size = 12),
    plot.subtitle = element_text(size = 10, color = "gray30"),
    axis.title = element_text(face = "bold", size = 9),
    panel.grid.minor = element_blank(),
    panel.grid.major.x = element_blank()
  )

alpha_plot_w


ggsave("weighted_alpha_values.png", plot = alpha_plot_w, width = 6, height = 4, units = "in", dpi = 300)
```


```{r}
# run elastic net with that alpha (still using class weights)
cv_elnet_w <- cv.glmnet(
  x = x_train, 
  y = y_train,
  family = "binomial",
  alpha = best_alpha_w,
  type.measure = "auc",
  nfolds = 10,
  weights = wts_train
)

# plot the cross-validation results
plot(cv_elnet_w)
title(main = paste("Elastic Net CV (Weighted, α =", best_alpha_w, ")"), line = 2.5)

png("elasticnet_plot_weighted.png", width = 800, height = 600, res = 150)
```


```{r}
# extracting the best lambda (penalty)
best_lambda_w <- cv_elnet_w$lambda.min
best_lambda_w

# view coefficients at best alpha/lambda
coef_elnet_w <- coef(cv_elnet_w, s = "lambda.min")
coef_elnet_w
```

For the same reasons as our previous Elastic Net attempt, because our alpha value stayed closely to 0, this was practically Ridge. So, while it handled multicollinearity of our variables, it did not shrink coefficients to zero, meaning there was no variable selection. We will now do Lasso with our weighted classes. 

# 7. Lasso (Weighted)

```{r}
# cross-validated Lasso with class weights
cv_lasso_w <- cv.glmnet(
  x = x_train,
  y = y_train,
  family = "binomial",
  alpha = 1,                 # Lasso
  type.measure = "auc",
  nfolds = 10,
  weights = wts_train        
)
```


```{r}
# plotting CV curve
plot(cv_lasso_w)
title(main = "Lasso Cross-Validation (Weighted, α = 1)", line = 2.5)
png("lasso_cv_plot_weighted.png", width = 800, height = 600, res = 150)

# Extract key lambdas
lambda_min_w <- cv_lasso_w$lambda.min
lambda_1se_w <- cv_lasso_w$lambda.1se
lambda_min_w; lambda_1se_w

```

Lambda min of 0.00022 yielded the highest AUC during traning, and lambda 1se = 0.00056 represents a simpler model within one standard error of the minimum. The cross-validation curve shows that the model performance steadily increase as lambda decreased, peaking near 0.64. The gap between lambda min and lambda 1se is small, indicating that the simpler lambda 1se model sacrifices minimal predictive performance while retaining fewer predictors. 

```{r}
# Coefficients & selected features at λ_min and λ_1se (weighted)
coef_min_w <- coef(cv_lasso_w, s = "lambda.min")
coef_1se_w <- coef(cv_lasso_w, s = "lambda.1se")

nonzero_min_w <- rownames(coef_min_w)[coef_min_w[, 1] != 0]
nonzero_1se_w <- rownames(coef_1se_w)[coef_1se_w[, 1] != 0]

nonzero_1se_w     # inspect
length(nonzero_1se_w); length(nonzero_min_w)
```

From the above output, we see our lambda 1se model yielded 43 nonzero predictors, a meaningful reduction from the 73 predictors retained by Elastic Net. 

```{r}
# test-set performance: λ_min vs λ_1se (weighted)
p_min_w <- as.numeric(predict(cv_lasso_w, newx = x_test, s = "lambda.min", type = "response"))
p_1se_w <- as.numeric(predict(cv_lasso_w, newx = x_test, s = "lambda.1se", type = "response"))

roc_min_w <- roc(y_test, p_min_w, quiet = TRUE)
roc_1se_w <- roc(y_test, p_1se_w, quiet = TRUE)

auc_min_w <- as.numeric(auc(roc_min_w))
auc_1se_w <- as.numeric(auc(roc_1se_w))
auc_min_w; auc_1se_w

# Plot both weighted ROC curves
plot(roc_min_w, col = "#D02130", lwd = 2,
     main = paste0("Test ROC — Weighted Lasso: λ_min vs λ_1se\n",
                   "AUC(min) = ", round(auc_min_w, 3),
                   " | AUC(1se) = ", round(auc_1se_w, 3)))
lines(roc_1se_w, col = "#1f77b4", lwd = 2)
legend("bottomright", legend = c("λ_min (weighted)", "λ_1se (weighted)"),
       col = c("#D02130", "#1f77b4"), lwd = 2, bty = "n")

```

The above graph shows the discriminatory ability of both the lambda min and lambda 1se model, showing how well they separate injured from non-injured jumps. The ROC curve plots sensitivity (true positive rate) vs. specificity (false positive rate). The AUC is a summary measure with 0.5 meaning random guessing and 1 being perfect discrimination. Because both lambda min and lambda 1se were only slightly above 0.5, this means our models struggle to separate the two groups on the test set. Similarly, looking at the ROC curves, we see both the red and the blue line hug diagonal, meaning the model's predictions are only slightly better than random. 

This altogether implies that injury risk is not well explained by a purely linear combination of these CMJ metrics. 


Yet, this still performed feature selection under penalized regularization. This means it removed noisy, redundant, and collinear predictors from the original 70+ CMJ metrics. The 43 variables it retained are those with the strongest linear association to injury. If we apply this subset to something like XGBoosting to perform the prediciton, this model does not require linearity and, instead, captures non-linear interactions automatically. We can apply this subset as the starting point for our future predictive model to avoid overfitting on noise from the original dataset, shrink training time and improve interpretability, and retian variables with provel importance. 

# 8. ??? bootstrapping + lasso and get a list of the top contributors to injury risk that appear in most samples